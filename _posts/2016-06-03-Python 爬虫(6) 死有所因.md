最近忙了起来，博客更新的速度慢了一些。希望忙过這一阵子可以保持持续更新的速度。此后还会开通相册功能，摄影爱好者的必要板块啊！


之前的爬虫学习，调试时进场遇到HTTTP错误状态吗，此篇总结一下的网页的访问异常处理。

异常一般发生在没有网络连接，对方服务器不存在的情况下。

传送门之--[**常见的http状态码**](http://www.w3school.com.cn/tags/html_ref_httpmessages.asp)

一般说来：

数字状态码
- 100 - 200 成功
- 4xx:问题出在自己客户端
- 5XX:问题出在服务器

首先看一下一个URL error的例子。


    import urllib.request
    import urllib.error
    
    req = urllib.request.Request('http://www.ada123.com')
    
    try:
        urllib.request.urlopen(req)
    except urllib.error.URLError as e:
            print (e.reason)

调试后结果为：
    
        [Errno 11001] getaddrinfo failed
        
因为這样一个域名不存在

http error是url error 的子类。

我们再对http的状态码进行捕获：
    
    req = urllib.request.Request('http://adastaybrave.com/2016')  
    
    try:
        response = urllib.request.urlopen(req)
    
    except urllib.error.HTTPError  as e:
            print (e.code)
            print (e.read().decode('utf-8'))
            
這里捕获了http error的状态码404.也打印出了未响应网页的内容。
![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)http.error.png)


### 捕获方法一：         
值得注意的是，我们处理异常的一般写法为(伪代码，表框架)：

    import urllib模块
    import urlib.error模块
    
    req = Request(URL)
    
    try：
        response =  urllopen(req)
    
    except urllib.error.HTTPError as e：
        print(e.code,其它提示内容)
    
    except urllib.error.URLError as e：
        print(e.reason,其它提示内容)

先捕获HTTPErrror,因为這是URL Error的子类。如果URLError在前面它会捕捉到所有的URLError
    
### 捕获方法二   
此外还有另一种方法，利用has attr属性

    try:
        response = urllib.request.urlopen(req)
    '''

    except urllib.error.URLError as e:
        
        if hasattr(e,'code'):
            print (e.code)
        elif hasattr(e, 'reason'):
            print (e.reason)
            
    else:    
        print ('No exception.') 

