---
layout: post
title:  "爬虫(5) 夕花朝拾"
categories: Python爬虫
tags:  Python
---

很多爬虫都会写怎么爬取图片，我看着也就写了一点。

不忘初心，爬虫是从网页上抓取数据，那么对网页的解析至关重要。

如果我们要怎么抓取一个有很多图片信息的网页怎么做呢？简单粗暴：



1.  **定位图片标签在html文件中的位置**
1. 提取图片的url
1. 下载图片到本地

## 必备工具之BeautifulSoup.
Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。
Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。。
這里贴一个关于BeautifulSoup的悦目一点的教程

---

[Python爬虫利器二之Beautiful Soup的用法](http://cuiqingcai.com/1319.html)

[官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#id17)

---

在安装过程中发现，常用的pip和install在3.5版本的Python中都封装好了。.exe文件什么的最喜欢了！在此基础上即可安装BeautifulSoup。

首先我们对目标网页做一个分析，我抓取的是這个[在树上唱歌的鸟儿萌！](http://www.imgmob.net/bird-on-a-branch-with-berries-wallpaper.html)（因为自己本身学摄影，喜欢這样的网站。此外我是妹子，不需要爬妹子图 \.\^o^）

---
## 如何分析网页元素？

打开目标网页，锁定我们要抓取的图片，我们发现這都是缩略图，一般该类网站的形式都是将完整图片所在的网页放在一个限定大小的<div>区块元素中，那么将鼠标移到一幅图片中
![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)QQ%E6%88%AA%E5%9B%BE20160524202154.jpg)
右键，审查元素，Chrome可快捷键Ctrl+shift+I跳转到调试醋窗口，如下图所示。
![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)href.jpg)
移动鼠标发现這些缩略图都被包裹在這个最大的<div>里
![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)all.jpg)
我们发现每一个缩略图都对应着一个同级别的<div>区块,Chrome很机智的可以让你看到你选择的区块，以淡蓝的颜色标出
![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)child_example.jpg)


在這个父<div>中下的许多同级的<div>，就是包裹起来的缩略图的区域；
![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)child_example.jpg)


我们看到，這些个同级子div下果然是一个a href标签，里面的img src后的内容复制在浏览器打开后就是原图，所以我们需要的就是拿到这些元素。

![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)href.jpg)


### 本实例需要的模块：
    from bs4 import  BeautifulSoup
    import urllib.request
    #html解释作用
    import urllib.parse
    #文件读写操作模块
    import os
    
## 实战
    
### 1、打开冰箱！
好吧，打开网页！不陌生吧！

    def url_open(url):
        req = urllib.request.Request(url)
        response = urllib.request.urlopen(req)
        html = response.read()
        return html

好了现在我们已经拿到了目标网页的html文件（注意我们没有解码，而是utf-8,因为soup会自动解析并转化输出）

### 2、找到大象
大象大象。。。不，我们要定位图片的地址了。

    def get_links(url):
        html = url_open(url)
        #解析html
        soup = BeautifulSoup(html, 'html.parser')
        # 定位父级<div>
        divs = soup('div', {'class':'mansoryItem'})
        #用来存储每一张img具体链接的列表
        img_addrs = []
    
    #拿到每一个子div中的img
    for div in divs:
        img_urls = div.find('img')['src']
        img_addrs.append(img_urls)
    return img_addrs


這里我打赢了div的内容，和img_urls。我们看看都是什么形式。总之，print一切你想要的ground truth

![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)divs.jpg)

---

![image](http://7xq62e.com1.z0.glb.clouddn.com/web_spider(2)img_linl.jpg)

下面还有何难度，参考之前的下载喵星人的教程，收到擒来的一切！

### 3.把大象装进冰箱！

接下来，我们就是以此访问img_addrs中的每一个图片的链接，作为req给得到它的二进制文件并写入。
    
    def save_imgs(folder, img_addrs):
        for each in img_addrs:
            #拿到被分割符/分开的最后一个字段，也就是文件名
            filename =  each.split('/')[-1]
            with open(filename, 'wb') as f:
                img =url_open(each)
                f.write(img)
                

###### 关好冰箱门

下面写下主函数啦。
    
    def download_birdpic(folder= 'bird_pic'):

        url = 'http://www.imgmob.net/bird-on-a-branch-with-berries-wallpaper.html'
    
        img_addrs = get_links(url)
        
        os.mkdir(folder)
        os.chdir(folder)
        
        save_imgs('bird_pic', img_addrs)
    
    if __name__ == '__main__':

        download_birdpic()
        
最终，你就会在你這个.py文件所在的目录下发现一个叫bird_pic的文件夹，里面就是你需要的图片!

也许会出现各种问题，503，502，404什么的，后续会对這些问题的形成做补充。

        

    





