---
layout: post
title:  "降低损失 (Reducing Loss)"
categories: MachineLearning
tags: MachineLearning
---

之前提到了损失，损失用来评估模型预测的准确程度，而机器学习训练的目标就是寻找损失最小下的权重和偏差。下面来说如何降低损失。

##梯度下降法
以迭代的方式降低损失。最常见的是梯度下降法。

**梯度**：与模型参数相关的误差函数的导数。
获得数据之后，计算這些数据的误差函数的梯度。

梯度下降法的第一个阶段是为w1选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为0或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：


![](http://7xq62e.com1.z0.glb.clouddn.com//pic/ML_1.png)

选择之后，我们计算损失曲线函数在起点处的梯度，梯度是偏导数的矢量;可以表示从哪个方向走回距离目标更接近或更远。

再次强调梯度是一个矢量，矢量就有方向和大小。梯度始终指向损失函数中增长最为迅猛的方向，而梯度下降算法会沿着负梯度方向走，尽快降低损失。梯度下降法依赖于负梯度。

![](http://7xq62e.com1.z0.glb.clouddn.com//pic/ML_2.png)

为了确定损失函数曲线上的下一个点，梯度下降算法会将梯度大小的一部分与起点相加。（矢量加法）
![](http://7xq62e.com1.z0.glb.clouddn.com//pic/ML_3.png)

##学习速率（步长）
梯度有方向，有大小，梯度下降算法将用梯度乘以一个步长（标量），来确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

在调参的过程中。需要花费很多时间调整学习速率，太小学习太慢，浪费时间。

![](http://7xq62e.com1.z0.glb.clouddn.com/ML_4.png)

而步长太大，下一个点可能就与目标点失之交臂
![](http://7xq62e.com1.z0.glb.clouddn.com/pic/ML_5.png)


所以要选择合适的步长
![](http://7xq62e.com1.z0.glb.clouddn.com/pic/ML_6.png)

##随机梯度下降法

面对大型海量数据执行梯度下降算法时，比如十几亿数千亿的样本。特征量是巨大的，我们把单词迭代中计算梯度的样本总数叫批量（Batch）. 批量较大时，单次迭代花费时间无法想象。

但是大型数据中一般都包含冗余数据，事实上批量越大，出现冗余可能性就越高。冗余是有助于消除杂乱的梯度的。我们从数据集中集中随机选择样本，通过小一点的数据集估算出大的平均值。

随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。
